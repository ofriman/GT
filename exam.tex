\documentclass[a4paper, 11pt, oneside]{article}

 \usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, float,centernot}
\usepackage[shortlabels]{enumitem}
 \usepackage[hidelinks]{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand{\charfusion}[3][\mathord]{
    #1{\ifx#1\mathop\vphantom{#2}\fi
        \mathpalette\mov@rlay{#2\cr#3}
      }
    \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother
\newcommand{\bigcupdot}{\charfusion[\mathop]{\bigcup}{\cdot}}

\newcommand\myeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}


\let\oldquote\quote
\let\endoldquote\endquote
\renewenvironment{quote}[2][]
  {\if\relax\detokenize{#1}\relax
     \def\quoteauthor{#2}%
   \else
     \def\quoteauthor{#2~---~#1}%
   \fi
   \oldquote}
  {\par\nobreak\smallskip\hfill(\quoteauthor)%
   \endoldquote\addvspace{\bigskipamount}}

\newtheorem{innercustomthm}{Theorem}
\newenvironment{theorem}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}
  
  \newtheorem{innercustomprob}{Problem}
\newenvironment{problem}[1]
  {\renewcommand\theinnercustomprob{#1}\innercustomprob}
  {\endinnercustomprob}
  
  \newtheorem{innercustomdef}{Definition}
\newenvironment{definition}[1]
  {\renewcommand\theinnercustomdef{#1}\innercustomdef}
  {\endinnercustomdef}
  
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand\abs[1]{\left|#1\right|}
\newcommand\imark[2]{\text{i-Mark}{(\{#1\},\{#2\})}}
\DeclareMathOperator\mex{mex}
\DeclareMathOperator\opt{opt}

\begin{document}

\title{Final take-home exam}
\author{Oren Friman 301677613}
\maketitle
				   
\begin{problem}{1}\label{problem1}
Show how to find the two non-zero eigenvalues of $K_{1,n}$ using the \textbf{functional}
approach. A natural alternative approach would be to memorise or dig up the spectrum of $K_{n,m}$
considered here; and then derive the answer from that. This will not be accepted as an answer.
\end{problem}

\begin{proof}
We will use \cite[Equation 1]{adjacency_matrix} to find  the two non-zero eigenvalues.
\begin{equation*}
\lambda x(u) = \sum_{v \sim u} x(v)
\end{equation*}
Now we must take $v_1$ otherwise the sum of the neighbors will be equal zero.
\begin{equation*}
\lambda x(v_1) = \sum_{v \sim v_1} x(v)
\end{equation*}
We will consider the simplest eigenvector  and we take $x_2 = \ldots = x_{n+1} = 1$.
\begin{equation*}
\lambda x_1 = n
\end{equation*}
having $\lambda = x_1 = \sqrt{n}$ gives us two eigenvalues $\pm \sqrt{n}$
\end{proof}		

\begin{problem}{2}\label{problem2}
Let $G$ be a graph. Is it true that $\lambda_n(G) < 0$ always hold? Prove your claims.
\end{problem}

\begin{proof}
No, this is not true, as $\sum_i \lambda_i = 0$ \cite{product_and_sum_of_eigenvalues}, this will be true only if $\lambda_1(G) \neq 0$.
Contradiction by example. Let G be a graph where $V(G) = \{v_1,v_2\}$ and $E(G) = \{\}$ Then
$A(G) = 
\begin{pmatrix}
0 & 0  \\
0& 0  \\
\end{pmatrix}$.

\begin{equation*}
det(A(G) - \lambda I) =
\begin{vmatrix}
-\lambda & 0            \\ 
0             & -\lambda \\
\end{vmatrix} = (-\lambda)^2
\end{equation*}
\end{proof}		
   
\begin{problem}{3}\label{problem3}\hfill 
  \begin{enumerate}[(a)]
  \item Let $H$ be a connected spanning subgraph of a (connected) graph $G (\text{i.e., }V (H) =
V (G))$. Prove that $\lambda_1(H) \leq \lambda_1(G)$
  \item Let $H$ be a connected subgraph of a connected graph $G$ such that $\abs{V (H)} < \abs{V (G)}$.
Prove that $\lambda_1(H) < \lambda_1(G)$.
\end{enumerate}
\end{problem}

\begin{proof}

  \begin{enumerate}[(a)]
  
  
  \item\label{problem3.1}
   \cite[Proposition 1.3.10]{introduction-spectra} prove this question.
  
\begin{quote}{\cite[Proposition 1.3.10]{introduction-spectra}}{If $G - uv$ is the graph obtained from a connected graph $G$ by deleting the edge $uv$, then $\lambda_1(G-uv) < \lambda_1(G)$.}
\hfill\break
Proof.
Let $x = 
\begin{pmatrix}
x_1 & \ldots & x_n
\end{pmatrix}^T
$ be a non-negative (\cite[Theorem 1]{perron_frobenius}) unit eigenvector of $G - uv$ corresponding to $\lambda_1(G - uv)$. Then
 \begin{equation*}
\lambda_1(G - uv) = x^TA(G-uv)x \leq x^TA(G)x \leq \lambda_1(G)
\end{equation*}
if $\lambda_1(G -uv) = \lambda_1(G)$ then $x$ is the principal eigenvector of $G$ and hence has no zero entries. Now $x^TA(G-uv)x = x^TA(G)x - 2x_ux_v  < \lambda_1 (G - uv)$, a contradiction.
\end{quote}


  \item
  \cite[Proposition 1.3.9]{introduction-spectra} prove this question.
\begin{quote}{\cite[Proposition 1.3.9]{introduction-spectra}}{For any vertex $u$ of a connected graph $G$, we have $\lambda_1(G-u) < \lambda_1(G)$.}
\hfill\break
Proof.
Let $A = 
\begin{pmatrix}
A' & r  \\
r^T & 0  \\
\end{pmatrix}
$, where $A' = A(G-u)$, and let $x$ be a unit eigenvector of $A'$ corresponding to $\lambda_1(G-u)$. if $y = \begin{pmatrix}
x \\
0 \\
\end{pmatrix}$
then $y^Ty = 1$ and $\lambda_1(G - u) = y^TAy \leq \lambda_1(G)$. if equality holds then y is an eigenvector of A corresponding to $\lambda_1(G)$; but this is a contradiction because $y$ has a zero entry.
\end{quote}
\end{enumerate}


\end{proof}		

\begin{problem}{4}\label{problem4}
Prove that
\begin{equation*}
max \Big\{ \overline{d}(G), \sqrt{\Delta(G)} \Big\} \leq \lambda_1(G) \leq \Delta(G)
\end{equation*}
where here $\overline{d}(G)$ is the average degree of G and $\Delta(G)$ is the maximum degree of G.
\end{problem}

\begin{proof}
 \cite[Proposition 1.2.2]{laszlo_background} prove this question.
\begin{quote}{ \cite[Proposition 1.2.2]{laszlo_background}}{For every graph $G$,  
\begin{equation*}
max \Big\{ \overline{d}, \sqrt{d_{\text{max}}} \Big\} \leq \lambda_1 \leq d_{\text{max}}
\end{equation*}}
\hfill\break
Proof.
Let $u = 
\begin{pmatrix}
1 / \sqrt{n}, & \ldots &, 1 / \sqrt{n} 
\end{pmatrix}^T
$ then 
\begin{equation*}
\lambda_1 \geq u^tA_Gu = \frac{1}{n} \sum_{i,j} A_{ij} = \frac{2m}{n} =  \overline{d}
\end{equation*}
Let $H$ denote the star with $d_{\text{max}}$ rays. Then $H \subseteq G$, and hence by  \cite[Proposition 1.2.1]{laszlo_background},
\begin{equation*}
\lambda_1 (G) \geq \lambda_1(H) = \sqrt{d_{\text{max}}}
\end{equation*}
Finally let $v_1$ denote the eigenvalue belonging to $\lambda_1$, and $v_1 = 
\begin{pmatrix}
u_{11}, & \ldots &, u_{1n}
\end{pmatrix}^T$. We may assume that $u_{11} \geq u_{12} \geq \ldots$ Then
\begin{equation*}
\lambda_1 v_{11} = \sum^n_{i=1}A_{1i}v_{1i} \leq  \sum^n_{i=1}A_{1i}v_{11} = d_1v_{11} \leq d_{\text{max}}v_{11} 
\end{equation*}
and hence $\lambda_1 \leq d_{\text{max}}$
\end{quote}
\end{proof}		

\begin{problem}{5}\label{problem5}
Let $G$ be a connected $d$-regular $n$-vertex graph and let $\lambda(G) := max_{2\leq i \leq n}\abs{\lambda_i}$ where the $\lambda_i$'s denote the eigenvalues of $G$. Prove that
\begin{equation*}
\left\lvert  e_G(X,Y) - \frac{d\abs{X}\abs{Y}}{n} \right\rvert \leq \lambda(G) \sqrt{  \abs{X}\abs{Y}  \left( 1 - \frac{\abs{X}}{n} \right)\left( 1 - \frac{\abs{Y}}{n} \right)} 
\end{equation*}
\underline{\textbf{for every}} $X,Y \subseteq V(G)$.
\end{problem}

\begin{proof}
 \cite[Lemma 8]{expander} prove this question.
\end{proof}		
 
 \begin{problem}{6}\label{problem6}
 Let $G$ and $\lambda(G)$ be as in \ref{problem5}. Prove that
 \begin{equation*}
 \lambda(G) \geq \sqrt{d}(1-o(1))
\end{equation*}
where $o(1)$ is a quantity that goes to zero as $n$ tends to infinity.
\end{problem}
\begin{proof}
We will use \ref{problem5} inequality.

\begin{equation*}
\left\lvert  e_G(X,Y) - \frac{d\abs{X}\abs{Y}}{n} \right\rvert \leq \lambda(G) \sqrt{  \abs{X}\abs{Y}  \left( 1 - \frac{\abs{X}}{n} \right)\left( 1 - \frac{\abs{Y}}{n} \right)} 
\end{equation*}
We will isolate $\lambda(G)$
\begin{equation*}
\frac{\left\lvert  e_G(X,Y) - \frac{d\abs{X}\abs{Y}}{n} \right\rvert}{\sqrt{  \abs{X}\abs{Y}  \left( 1 - \frac{\abs{X}}{n} \right)\left( 1 - \frac{\abs{Y}}{n} \right)} } \leq \lambda(G)
\end{equation*}
Recall that $G$ and $\lambda(G)$ are the same as in \ref{problem5}. 
\begin{align*}
\lambda(G) \geq &
\frac{\left\lvert  e_G(X,Y) - \frac{d\abs{X}\abs{Y}}{n} \right\rvert}{\sqrt{  \abs{X}\abs{Y}  \left( 1 - \frac{\abs{X}}{n} \right)\left( 1 - \frac{\abs{Y}}{n} \right)} }
\\ = & \sqrt{d}\left(\frac{\left\lvert  e_G(X,Y) - \frac{d\abs{X}\abs{Y}}{n} \right\rvert}{\sqrt{ d \abs{X}\abs{Y}  \left( 1 - \frac{\abs{X}}{n} \right)\left( 1 - \frac{\abs{Y}}{n} \right)} }\right)
\end{align*}
Notice that $0 \leq e_G(X,Y) \leq d \abs{X}$
\begin{align*}
\lambda(G) \geq &
\sqrt{d}\left(\frac{\left\lvert   d \abs{X} - \frac{d\abs{X}\abs{Y}}{n} \right\rvert}{\sqrt{ d \abs{X}\abs{Y}  \left( 1 - \frac{\abs{X}}{n} \right)\left( 1 - \frac{\abs{Y}}{n} \right)} }\right) \\ = &
\sqrt{d}\left(\frac{\sqrt{ d \abs{X}} \left\lvert  1 - \frac{\abs{Y}}{n} \right\rvert}{\sqrt{\abs{Y}  \left( 1 - \frac{\abs{X}}{n} \right)\left( 1 - \frac{\abs{Y}}{n} \right)} }\right)\\ = &
\sqrt{d}\left(\frac{\sqrt{ d \abs{X} \left\lvert  1 - \frac{\abs{Y}}{n} \right\rvert}}{\sqrt{\abs{Y} \left( 1 - \frac{\abs{X}}{n} \right)} }\right) \\ \geq & 
\sqrt{d} \left(1 - 	\frac{1}{\sqrt{n}}\right) \quad \text{(???)}
\end{align*}
\end{proof}		

 \begin{problem}{7}\label{problem7}
 Let $G$ be a connected $d$-regular $n$-vertex graph.
 \begin{enumerate}
 \item Prove that for any two sets $X,Y \subseteq V(G)$ satisfying $\abs{X},\abs{Y} \geq \lambda(G)n/d$ it holds that $e_G(X,Y)\geq 1$.
 \item Prove that for every $\varepsilon > 0$ and every set $W \subseteq V(G)$ there are at most $\frac{(\lambda(G)n)^2}{(\varepsilon d)^2\abs{W}}$ vertices with less than $(1 - \varepsilon)\abs{W}d/n$ neighbors in $W$.
 \item Let $X,Y,Z \subseteq V(G)$ such that $\abs{X} \geq 1 + 4\lambda(G)$ and $\abs{Y},\abs{Z} \geq 2\lambda(G)n^2/d^2$. Then there exists a triangle in $G$ with one vertex in each of the sets $X,Y$ and $Z$. 
 \end{enumerate}
 
\end{problem}

\hfill\break

 \begin{enumerate}
 \item
 \begin{proof}
 We will use \ref{problem5} inequality.

\begin{equation*}
\left\lvert  e_G(X,Y) - \frac{d\abs{X}\abs{Y}}{n} \right\rvert \leq \lambda(G) \sqrt{  \abs{X}\abs{Y}  \left( 1 - \frac{\abs{X}}{n} \right)\left( 1 - \frac{\abs{Y}}{n} \right)} 
\end{equation*}	
   
We can set $\abs{X} = \lambda(G)n/d,\abs{Y} = \lambda(G)n/d$.
  
 \begin{equation*}
\left\lvert  e_G(X,Y) - \frac{d\lambda(G)\frac{n}{d}\lambda(G)\frac{n}{d}}{n} \right\rvert \leq \lambda(G) \sqrt{  \lambda(G)\frac{n}{d}\lambda(G)\frac{n}{d}  \left( 1 - \frac{\lambda(G)\frac{n}{d}}{n} \right)\left( 1 - \frac{\lambda(G)\frac{n}{d}}{n} \right)} 
\end{equation*}	

Simplify equation

 \begin{equation*}
\left\lvert  e_G(X,Y) - \frac{d\left(\lambda(G)\frac{n}{d}\right)^2}{n} \right\rvert \leq \lambda(G)^2\frac{n}{d}\left( 1 - \frac{\lambda(G)\frac{n}{d}}{n} \right)
\end{equation*}	

remove absolute value 

\begin{equation*}
-\lambda(G)^2\frac{n}{d}\left( 1 - \frac{\lambda(G)\frac{n}{d}}{n} \right) \leq  e_G(X,Y) - \frac{d\left(\lambda(G)\frac{n}{d}\right)^2}{n} \leq \lambda(G)^2\frac{n}{d}\left( 1 - \frac{\lambda(G)\frac{n}{d}}{n} \right)
\end{equation*}	

Let look at the first inequality

\begin{equation*}
-\lambda(G)^2\frac{n}{d}\left( 1 - \frac{\lambda(G)\frac{n}{d}}{n} \right) \leq  e_G(X,Y) - \frac{d\left(\lambda(G)\frac{n}{d}\right)^2}{n}
\end{equation*}	

Now we can isolate $e_G(X,Y)$

\begin{align*}
e_G(X,Y) & \geq
-\lambda(G)^2\frac{n}{d}\left( 1 - \frac{\lambda(G)\frac{n}{d}}{n} \right) + \frac{d\left(\lambda(G)\frac{n}{d}\right)^2}{n}   \\ &=
-\lambda(G)^2\frac{n}{d} + \lambda(G)^3 \frac{n}{d^2} + \lambda(G)^2 \frac{n}{d} \\&=
 \lambda(G)^3 \frac{n}{d^2} \quad\tiny\text{( $\lambda(G) := max_{2\leq i \leq n}\abs{\lambda_i} $ together with \cite[Theorem 1]{perron_frobenius} and \cite[Lemma 13]{basic_spec} implies that $\lambda(G) = d$ )}  \\&\geq
 d n \geq 1
\end{align*}	
\end{proof}
 \item
  \begin{proof}

	\color{red} maybe same as above

\end{proof}

 \item
   \begin{proof}

		\color{red} no idea

\end{proof}
 \end{enumerate}
		

 \begin{problem}{8}\label{problem8}
 Prove that if a connected $d$-regular $n$-vertex graph satisfies
 \begin{equation*}
\abs{N_G(S)} := \abs{\{ v \in V(G) : \exists u \in S \text{ s.t. } uv\in E(G)\}} \geq (1 + k)\abs{S}
\end{equation*}
for every $S \subseteq V(G)$, $\abs{S} \leq n/2$, then
 \begin{equation*}
d - \lambda(G) \geq \frac{k^2}{2d}
\end{equation*}
\end{problem}
\begin{proof}

\color{red} no idea

\end{proof}		

 \begin{problem}{9}\label{problem9}
 Let $M$ be an irreducible Markov chain with state space $\Omega$ and transition matrix $P$. Prove that if $M$ has period $d > 1$ then one can write
 \begin{equation}\label{9e1}
\Omega = \bigcupdot_{i = 0}^{d-1} C_i
\end{equation} 
such that for every $i \in C_k$, $k = 0, \ldots , d - 1$
\begin{equation*}
\sum_{j \in C_{k+1}} P_{ij} = 1
\end{equation*} 
(where here $C_d = C_0$). Moreover, the decomposition of (\ref{9e1}) is unique.
\end{problem}
\begin{proof}

\cite[Theorem 4.1]{pierre_markov} prove this question.
  
\begin{quote}{\cite[Theorem 4.1]{pierre_markov} }{(Cyclic Structure)}
For any irreducible Markov chain, one can find a unique partition of E into $d$ classes $C_0, C_1, \ldots, C_{d-1}$ such that for all $k,i \in C_k$,
\begin{equation*}
\sum_{j \in C_{k+1}} P_{ij} = 1
\end{equation*} 
where by convention $C_d = C_0$, and where $d$ is maximal (that is, there is no other such partition $C'_0, C'_1, \ldots, C'_{d'-1}$ with $d' > d$)
\hfill\break
Proof.
``A direct consequence of Theorem 4.3 below''
\end{quote}

\begin{quote}{\cite[Theorem 4.3]{pierre_markov} }({Lattice Theorem)}
Let P be an irreducible stochastic matrix with period $d$. Then for all states $i,j$ there exist $m \geq 0$ and $n_0 \geq 0$ ($m$ and $n_0$ possibly depending on $i, j$) such that
\begin{equation*} 
p_{i,j}(m+nd) > 0, \forall n \geq n_0
\end{equation*} 
\hfill\break
Proof.
First observe that it suffices to prove this for $i = j$. Indeed, there exists $m$ such that $p_{ij}(m) > 0$, because $j$ is accessible from $i$, the chain being irreducible, and therefore, it for some $n_0 \geq 0$ we have $p_{ij}(nd) > 0$ for all $n \geq n_0$, then $p_{ij}(m + nd) \geq p_{ij}(m)p_{jj}(nd) > 0$ for all $n \geq n_0$. The gcd of the set $A = \{k \geq 1 \mid p_{ij}(k) > 0\}$ is $d$, and $A$ is closed under addition. The set $A$ therefore contains all but a finite nunmber of the positive multiples of $d$ (see Theorem 1.1 of the Appendix). In other words, there exists $n_0$
 such that $n > n_0$ implies $p_{jj} (nd) > 0$\end{quote}
\end{proof}		

 \begin{problem}{10}\label{problem10}
 Let $M$ be a Markov chain with transition matrix $P$ and state space $\Omega$.
 For $i,j \in \Omega$ and $n \in \Z^{+} define$
 \begin{equation*} 
f^{(n)}_{ij} := Pr[T_j=n\mid X_0=i]
\end{equation*} 
 \begin{enumerate}
 \item Prove that for all $n \geq 1$
  \begin{equation*} 
P^{(n)}_{ij} = \sum_{m=1}^n f_{ij}^{(m)}P_{ij}^{n-m}
\end{equation*} 

 \item Let $j \in \Omega$ be transient. Prove that
  \begin{equation*} 
\lim_{n\to\infty} P_{ij}^{(n)} = 0
\end{equation*} 
for all $i \in \Omega$
  \end{enumerate}

\end{problem}
 \begin{enumerate}
 \item
\begin{proof}
In \cite[Theorem 12.33]{markov} they used the extended Markov property to obtain the same result.

\begin{quote}{\cite[Theorem 12.7]{markov} }{(Extended Markov prop)}
Let $X$ be a Markov chain. For $n \geq 0$, for any event $H$ given in terms of the past history $X_0, X_1, \ldots, X_{n-1},$ and any event $F$ given in terms of the future $X_{n+1}, X_{n+2}, \ldots,$
\begin{equation*}
P(F \mid X_n = i, H) = P(F \mid X_n = i) \quad \text{for } i \in S
\end{equation*} 
\end{quote}

\begin{quote}{\cite[Theorem 12.33]{markov} }{}
Using conditional probability and the extended Markov property, Theorem 12.7, for $n \geq 1$,
 \begin{equation}\label{12d34}
p_{i,j}(n) = \sum^{\infty}_{m=1}P_i (X_n = j \mid T_j = m)P_i(T_j =m).
\end{equation} 
The summand is 0 for $m > n$, since in this case the first passage to $j$ has not taken place by time $n$. For $m \leq n$, 
\begin{equation*} 
P_i(X_n = j \mid T_j=m)=P_i(X_n = j \mid X_m = j, H),
\end{equation*} 
where $H = \{X_r \neq j \text{ for } 1\leq r < m \}$ is an event defined prior to time $m$. By the extended Markov property,
\begin{equation*} 
P_i(X_n = j \mid T_j=m)=P(X_n = j \mid X_m = j) = P_j(X_{n-m}=j).
\end{equation*} 
We substitute this into (\ref{12d34}) to obtain
\begin{equation*} 
p_{ij}(n) = \sum_{m=1}^n p_{ij}(n-m) f_{ij}(m)
\end{equation*} 
\end{quote}
\end{proof}		

 \item
\begin{proof}
$j$ is transient and so it follow that Pr$[X_n = x\text{ for } \infty \text{ many } n] < 1$ \cite{markov_elad}. We can deduce that the probability of going from state i to state j in infinity time steps is 0.

{\tiny\color{red} only intuition}
\end{proof}		
  \end{enumerate}
  
 \begin{problem}{11}\label{problem11}
\end{problem}
\begin{proof}

\end{proof}		

\begin{thebibliography}{9} 
\bibitem{adjacency_matrix}
Elad Aigner-Horev,
The adjacency matrix of a graph.
\\\texttt{http://www.elad-horev.org/Expositions/Adjacency\_Matrix.pdf}

\bibitem{product_and_sum_of_eigenvalues}
Steven Zheng,
Product and Sum of Eigenvalues.
\\\texttt{https://brilliant.org/discussions/thread/product-and-sum-of-eigenvalues/}

\bibitem{perron_frobenius}
Elad Aigner-Horev,
The Perron-Frobenius theorem for connected graphs.
\\\texttt{http://www.elad-horev.org/Expositions/Perron\_Frobenius.pdf}

\bibitem{introduction-spectra}
Dragos Cvetkovic, Peter Rowlinson,
Introduction-Spectra-Mathematical-Society-Student (Kindle Edition)

\bibitem{laszlo_background}
laszlo lovasz and Katalin Vesztergombi,
Background material.
\\\texttt{https://pdfs.semanticscholar.org/6ba8/74424408264214220c2785d4bdf521a9dd9d.pdf}
Background material

\bibitem{expander}
Elad Aigner-Horev,
The adjacency matrix of a graph.
\\\texttt{http://www.elad-horev.org/Expositions/expander\_mixing\_lemma.pdf}

\bibitem{basic_spec}
Elad Aigner-Horev,
Information from the spectrum of the adjacency matrix.
\\\texttt{http://www.elad-horev.org/Expositions/basic\_info\_spec.pdf}

\bibitem{pierre_markov}
Pierre Bremaud,
Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues.

\bibitem{markov}
Markov chains.
\\\texttt{http://www.statslab.cam.ac.uk/~grg/teaching/chapter12.pdf}

\bibitem{markov_elad}
Elad Aigner-Horev,
Classifying states of Markov chains.
\\\texttt{http://www.elad-horev.org/Expositions/Markov1\_5.pdf}
\end{thebibliography}

\end{document}
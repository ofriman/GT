\documentclass[a4paper, 11pt, oneside]{article}

 \usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, float, enumitem}
 
\newtheorem{innercustomthm}{Theorem}
\newenvironment{theorem}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}
  
  \newtheorem{innercustomprob}{Problem}
\newenvironment{problem}[1]
  {\renewcommand\theinnercustomprob{#1}\innercustomprob}
  {\endinnercustomprob}
  
  \newtheorem{innercustomdef}{Definition}
\newenvironment{definition}[1]
  {\renewcommand\theinnercustomdef{#1}\innercustomdef}
  {\endinnercustomdef}
  
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand\abs[1]{\left|#1\right|}
\newcommand\imark[2]{\text{i-Mark}{(\{#1\},\{#2\})}}
\DeclareMathOperator\mex{mex}
\DeclareMathOperator\opt{opt}

\begin{document}

\title{Advanced topics in graph theory Assignments}
\author{Oren Friman 301677613}
\maketitle

\section{Total unimodularity and incidence matrices}

\begin{problem}{1.1}\label{problem1.1}
Let $G$ be an undirected graph and let $\sigma$ be an orientation of its edges. Prove that
$\mathcal{Q}(G^\sigma)$ is totally unimodular.
\end{problem}

\begin{proof}
Let $N_{k\times k}$ be a square submatrix of $\mathcal{Q}(G^\sigma)$. We proceed by induction on $N$. For the base case $k = 1$, the determinant of such matrix is the number itself (-1, 0, 1). Suppose now that the theorem holds up to $k=n$. Here we consider cases:
\begin{enumerate}[label=(\alph*)]
\item $N$ contain at least one column or row $c_i$ containing only zero elements. The determinant of $N$ can be computed by using the Laplace expansion along $c_i$ which will give us zero.
\item $N$ contain at least one column or row $c_i$ containing only one element $e_j$ not equal zero. The determinant of $N$ can be computed by using the Laplace expansion along $c_i$ which will give us $(-1)^{i+ 1} * e_j * det(N_{i,j})$ where $N_{i,j}$ is submatrix of $N$, from the induction hypothesis $det(N_{i,j}) \in\{-1, 0, 1\}$ and $e_j \in\{-1, 1\}$ therefore the theorem holds.
\item $N$ is incidence matrix, we know that the sum of rows are equal zero therefore the rows are dependent and the determinant is equal zero.
\end{enumerate}
\end{proof}

\begin{problem}{1.2}
\hfill 
\begin{enumerate}[label=1.2.\arabic*]
\item \label{problem1.2.1} Let $n \geq 3$. Prove that
\begin{equation*}
det M(C_n) = 
	 \begin{cases}
	0, & \text{if $n$ is even}\\
	2, & \text{if $n$ is odd};
	 \end{cases}
\end{equation*}
\begin{proof}
\begin{multline*}
  det M(C_n) = 
  \underbrace {
\begin{vmatrix}
1 & 0 &  \cdots & 1 \\ 
1 & 1 &  \cdots & 0 \\
0 & 1 &  \cdots & 0 \\
\vdots & \vdots & \ddots &\vdots \\
0 & 0 & \cdots & 0 \\ 
0 & 0 & \cdots & 1 \\ 
\end{vmatrix}_{n\times n}
}_\text{we will expand along first row }
= \\
\underbrace {
\begin{vmatrix}
1 & 0 &  \cdots & 0 \\ 
1 & 1 &  \cdots & 0 \\
0 & 1 &  \cdots & 0 \\
\vdots & \vdots & \ddots &\vdots \\
0 & 0 & \cdots & 0 \\ 
0 & 0 & \cdots & 1 \\ 
\end{vmatrix}_{n - 1 \times n - 1}
}_\text{we will expand along first row }
 + (-1)^{n+1}
 \underbrace {
\begin{vmatrix}
1 & 1 &  \cdots & 0 \\ 
0 & 1 &  \cdots & 0 \\
0 & 0 &  \cdots & 0 \\
\vdots & \vdots & \ddots &\vdots \\
0 & 0 & \cdots & 1 \\ 
0 & 0 & \cdots & 1 \\ 
\end{vmatrix}_{n - 1 \times n - 1}
}_\text{we will expand along first column }
= \\ \cdots =
\begin{vmatrix}
1 & 0 \\ 
1 & 1 \\ 
\end{vmatrix}
+ (-1)^{n+1}
\begin{vmatrix}
1 & 1 \\ 
0 & 1 \\ 
\end{vmatrix}
= 1 + (-1)^{n+1} 
\end{multline*}
\end{proof}

\item Use the first part of this problem to prove that an undirected graph $G$ is bipartite if and if $M(G)$ is totally unimodular.
\begin{proof}
$G$ is bipartite $\rightarrow$ $M(G)$ is totally unimodular.\\ 
Let $G$ be bipartite graph and Let $N_{k\times k}$ be a square submatrix of $M(G)$. We proceed by induction on $N$. For the base case let $k = 1$, the determinant of such matrix is the number itself. Suppose now that the theorem holds up to $k=n$. Here we consider cases:
\begin{enumerate}[label=(\alph*)]
\item $N$ contain at least one column or row $c_i$ containing only zero elements. The determinant of $N$ can be computed by using the Laplace expansion along $c_i$ which will give us zero.
\item $N$ contain at least one column or row $c_i$ containing only one element equal 1. The determinant of $N$ can be computed by using the Laplace expansion along $c_i$ which will give us $(-1)^{i+ 1} * det(N_{i,j})$ where $N_{i,j}$ is submatrix of $N$, from the induction hypothesis $det(N_{i,j}) \in\{-1, 0, 1\}$ therefore the theorem holds.
\item $N$ is incidence matrix, we know from \cite[Theorem 10]{konig_egervary}  that the rk $N \leq n - 1$ therefore the columns are dependent and the determinant is equal zero.
\end{enumerate}
$M(G)$ is totally unimodular $\rightarrow$ $G$ is bipartite.\\ 
From K$\ddot{o}$nig Theorem, we know that a graph is bipartite if and only if it has no odd cycle. Also we saw in \ref{problem1.2.1} that  totally unimodular matrix cannot contain odd cycle therefore it most be bipartite.
\end{proof}
\end{enumerate}
\end{problem}

\section{The spectrum of some special graphs}

\begin{problem}{2.1}\label{problem2.1}
The graphs $C_4\cup K_1$ and $K_{1,4}$ are clearly non-isomorphic. Prove these are cospectral by proving that 
\begin{equation*}
\sigma(C_4\cup K_1) = \sigma(K_{1,4}) = 2^1, 0^3, (-2)^1.
\end{equation*}
\end{problem}

\begin{proof}
Lets start by calculate $\sigma(C_4\cup K_1)$
\begin{equation*}
det (C_4\cup K_1 - \lambda I) =
\begin{vmatrix}
-\lambda & 1             & 0             & 1                        & 0 \\ 
1             & -\lambda & 1             & 0                         & 0 \\ 
0             & 1             & -\lambda & 1                         & 0 \\ 
1             & 0             & 1             & -\lambda             & 0 \\ 
0             & 0             & 0             & 0                         & -\lambda \\ 
\end{vmatrix} =
-\lambda
\begin{vmatrix}
-\lambda & 1             & 0             & 1                         \\ 
1             & -\lambda & 1             & 0                          \\ 
0             & 1             & -\lambda & 1                          \\ 
1             & 0             & 1             & -\lambda             \\ 
\end{vmatrix} =-\lambda  det (C_4 - \lambda I)
\end{equation*}
So there is one eigenvalue 0 with multiplicity 1 and we also know from \cite[Example 7]{adjacency_matrix}  that the eigenvalues of $C_4$ are given by $\{2cos(\frac{2\pi k}{4}) \mid k \in [0, 3]\} = \{ 2, 0, -2, 0 \}$ each with multiplicity 1.
Second lets calculate $\sigma(K_{1,4})$, we know from \cite[Example 9]{adjacency_matrix} that $K_{1,3}$ has precisely two non zero eigenvalues $\pm \sqrt{4}$ and that 0 is an eigenvalue with multiplicity 3.
\end{proof}

\begin{problem}{2.2.1}\label{problem2.2.1}
The null graph or the empty graph is an n-vertex graph with no edges. Determine its spectrum.
\end{problem}

\begin{proof}
Let $A_n$ be the empty graph with n vertices.
\begin{equation*}
det(A_n - \lambda I) =
\begin{vmatrix}
-\lambda & 0             &  \cdots                     & 0    & 0 \\ 
0             & -\lambda &  \cdots & 0               & 0\\
0             & 0             &  \cdots & 0               & 0 \\
\vdots      & \vdots     & \ddots  &\vdots         &\vdots \\
0             & 0             & \cdots  &   -\lambda  & 0 \\ 
0             & 0             & \cdots  & 0                & -\lambda   \\ 
\end{vmatrix} = (-\lambda)^n
\end{equation*}
The spectrum of the empty graph is $0^n$.
\end{proof}

\begin{problem}{2.2.2}\label{problem2.2.2}
Determine $\sigma(K_n)$ using the fact that the null graph on n-vertices is the complement of $K_n$.
\end{problem}

\begin{proof}
Let $A_n$ the empty graph, we know that $A_n$ is $0$ regular graph and $K_n$ is the complement of $A_n$, from \cite[Proposition 10]{adjacency_matrix} we know the characteristic polynomial of the complement r regular graph.
\begin{equation*}
p_{K_n}(x) = (-1)^n\frac{x - n + 1}{x + 1} p_A(-x -1) = (-1)^n\frac{x - n + 1}{x + 1} ( -x -1)^n = (x - n + 1)  (x + 1)^{n-1}
\end{equation*}
The spectrum of $K_n$ is $n-1, (-1)^{n-1}$.
\end{proof}

\begin{problem}{2.3}\label{problem2.3}
Determine $\sigma(K_{n_1,n_2})$ by viewing it as the join of two empty graphs one with $n_1$ vertices and the other with $n_2$ vertices.
\end{problem}

\begin{proof}
Let $A_n$  be the empty graph.
From \cite[Lemma 11]{adjacency_matrix} we know the characteristic polynomial of the join between $n_1$ and $n_2$ vertices graphs.
\begin{align*}
\begin{split}
p_{K_{n_1,n_2}}(x) = & (-1)^{n_2} p_{A_{n_1}}(x)   p_{\overline{A_{n_2}}}(-x-1) + \\
								   & (-1)^{n_1} p_{A_{n_2}} (x)  p_{\overline{A_{n_1}}}(-x-1) -  \\
								   &  (-1)^{n_1 + n_2} p_{\overline{A_{n_1}}}(-x-1) p_{\overline{A_{n_2}}}(-x-1)
\end{split}
\end{align*}
From\ref{problem2.2.2} we know that the complement graph of the empty graph $A_n$ is $K_n$.
\begin{align*}
\begin{split}
 p_{K_{n_1,n_2}}(x)  = & (-1)^{n_2} p_{A_{n_1}}(x)   p_{K_{n_2}}(-x-1) + \\
								   & (-1)^{n_1} p_{A_{n_2}} (x)  p_{K_{n_1}}(-x-1) -  \\
								   &  (-1)^{n_1 + n_2} p_{K_{n_1}}(-x-1) p_{K_{n_2}}(-x-1)
\end{split}
\end{align*}
From\ref{problem2.2.1} we know the characteristic polynomial of the empty graph, from \cite[Example 4]{adjacency_matrix} and \ref{problem2.2.2} we know the characteristic polynomial of $K_n$.
\begin{align*}
\begin{split}
   p_{K_{n_1,n_2}}(x)   = & (-1)^{n_2} x^{n_1}   (-x - n_2)(-x) ^{n_2 - 1} + \\
								   & (-1)^{n_1} x^{n_2}  (-x - n_1)(-x) ^{n_1 - 1}-  \\
								   &  (-1)^{n_1 + n_2} (-x - n_1)(-x) ^{n_1 - 1} (-x - n_2)(-x) ^{n_2 - 1}
\end{split}
\end{align*}
\begin{multline*}
p_{K_{n_1,n_2}}(x) = (x + n_2) x ^ {n_1 + n_2 -1} + (x + n_1) x ^ {n_1 + n_2 -1} - (x + n_1)(x + n_2)x^{n_1 + n_2 - 2} =\\
x ^ {n_1 + n_2 - 2} [ x(x + n_2) + x(x + n_1) - (x + n_1)(x + n_2)] =\\
x ^ {n_1 + n_2 - 2}  (x^2 - n_1n_2)
\end{multline*}
The spectrum of $K_{n_1,n_2}$ is $0 ^ {n_1 + n_2 - 2}, \pm\sqrt{n_1n_2}$.
\end{proof}

\section{Connectivity and spectrum}

\begin{problem}{3.1}\label{problem3.1}
The spectrum of a graph is the union of the spectrum of its connected components where for multiplicities are added whenever there is a shared eigenvalue between components.
\end{problem}

\begin{proof}
Notice that
\begin{equation*}
\begin{pmatrix}
A(G) - \lambda I_{\abs{V(G)}} & 0 \\ 
0             & A(H) - \lambda I_{\abs{V(H)}} \\
\end{pmatrix} =
\begin{pmatrix}
A(G) - \lambda I_{\abs{V(G)}} & 0 \\ 
0             &  I_{\abs{V(H)}} \\
\end{pmatrix}
\begin{pmatrix}
 I_{\abs{V(G)}} & 0 \\ 
0             & A(H) - \lambda I_{\abs{V(H)}} \\
\end{pmatrix}
\end{equation*}
For square matrices A and B of equal size, $\det(AB)=\det(A)\det(B)$ \cite{determinant}.
Lets calculate the spectrum of $A(G\cup H)$ 
\begin{multline*}
\begin{vmatrix}
A(G) - \lambda I_{\abs{V(G)}} & 0 \\ 
0             & A(H) - \lambda I_{\abs{V(H)}} \\
\end{vmatrix} =
\begin{vmatrix}
A(G) - \lambda I_{\abs{V(G)}} & 0 \\ 
0             &  I_{\abs{V(H)}} \\
\end{vmatrix}
\begin{vmatrix}
 I_{\abs{V(G)}} & 0 \\ 
0             & A(H) - \lambda I_{\abs{V(H)}} \\
\end{vmatrix} =\\
\begin{vmatrix}
A(G) - \lambda I_{\abs{V(G)}}\\
\end{vmatrix}
\begin{vmatrix}
A(H) - \lambda I_{\abs{V(H)}} \\
\end{vmatrix}
\end{multline*}

\end{proof}

\begin{problem}{3.2}\label{problem3.2}
An irreducible real symmetric matrix $L$ is then a matrix that cannot be conjugated into block upper-triangular from; that is for every permutation matrix $P$ conformable with $L$ we have that
\begin{equation*}
P^{-1}LP\neq
\begin{pmatrix}
A  	& B \\ 
0       & C\\
\end{pmatrix} 
\end{equation*}
\end{problem}

\begin{proof}
\begin{equation*}
(P^{-1}LP)_{i,j} = (P^{T}LP)_{i,j} = \Sigma_{u=1}^n (\Sigma_{k=1}^n P^T_{i,k} L_{k,u}) P_{u,j} 
\end{equation*}
Notice that $i$ must equal $j$ to have a chance to not equal zero, therefore the only case is for $B$ to be equal to the zero matrix but we know that $L$ is irreducible therefore the claim holds.
\end{proof}

\begin{problem}{3.3}\label{problem3.3}
Let $M$ be a real symmetric matrix. Then $M$ is irreducible if and only if $G^M$ is connected.
\end{problem}

\begin{proof}
 $M$ is irreducible $\rightarrow$  $G^M$ is connected.
 Assume $M$ is reducible, then by \cite{connectivity_spec} there is non-trivial partition $U\cup W = [n]$ such that $M_{i,j} = 0$ for all $(i,j)\in U \times W$ .
 Recall that $M$ is symmetric so  $M_{j,i} = 0$. Notice that there will be no edges in $G^M$ between elements in $U$ and elements in $W$,  so $G^M$ must be disconnect.
On the other hand,  $G^M$ is connected $\rightarrow$ $M$ is irreducible.
Assume $G^M$ is disconnected, and let $C_1, \ldots, C_{c(G^M)}$ denote the connected components of $G^M$ Then $G^M$ has the form
\begin{equation*}
\begin{pmatrix}
C_1 & 0 &  \cdots & 0 \\ 
0 & C_2 &  \cdots & 0 \\
\vdots & \vdots & \ddots &\vdots \\
0 & 0 & \cdots & C_{c(G^M)} \\ 
\end{pmatrix}
\end{equation*}
Notice that between any two vertices $v, u \in V(G^M)$ where $v \in C_i$ and $u \in C_j$ such that $i \neq j$ there is no $u-v$ walk,
it follow that $M$ is reducible. 
\end{proof}

\begin{problem}{3.4}\label{problem3.4}
Let $M$ be a real symmetric non-negative matrix of order $n$. Then $M$ is irreducible if and only if for every $i,j\in [n]$ there exists a $k > 0$ such that $(M^k)_{i,j}>0$.
\end{problem}

\begin{proof}
We saw in \ref{problem3.3} that $M$ is irreducible if and only if $G^M$ connected. 
We know from \cite[Lemma 1]{basic_info_spec} that 
\begin{equation*}
A^r_{uv} = \text{number of u − v-r-walks in G}. 
\end{equation*}
A graph is connected when there is a path between every pair of vertices \cite{connectivity} and so for any two vertices $u, v \in A(G^M)$ there must be $0 < k \in \N$  such that there exist k − u-v-walks therefore the claim holds.
\end{proof}

\begin{problem}{3.5}\label{problem3.5}
A graph is connected if and only if its index is simple with a strictly positive eigenvector.
\end{problem}

\begin{proof}
If a garph is connected then by the Perron-Frobenius Theorem \cite{perron_frobenius} its index is simple with a strictly positive eigenvector.
On the other hand let $G$ be a graph that  its index is simple with a strictly positive eigenvector.
We continue by contradiction, let $G$ be a disconnected graph then from \ref{problem3.3} $G$ is reducible, and so $G$ as the form
$\begin{pmatrix}
X & 0 \\ 
0 & Y \\ 
\end{pmatrix}$
where $X$ and $Y$ are square matrices. we can write $x = (z_1,z_2)$.
\begin{equation*}
\begin{pmatrix}
X z_1 \\ 
Y z_2\\ 
\end{pmatrix} =
\begin{pmatrix}
X & 0 \\ 
0 & Y \\ 
\end{pmatrix} x = Ax = \lambda x =
\begin{pmatrix}
\lambda z_1 \\ 
\lambda z_2\\ 
\end{pmatrix}
\end{equation*}
recall from our assumption that $z_i > 0$.
\begin{equation*}
\begin{pmatrix}
X \abs{z_1} \\ 
Y \abs{z_2}\\ 
\end{pmatrix} =
\begin{pmatrix}
X z_1 \\ 
Y z_2\\ 
\end{pmatrix} =
\begin{pmatrix}
\lambda z_1 \\ 
\lambda z_2\\ 
\end{pmatrix} =
\begin{pmatrix}
\lambda \abs{z_1} \\ 
\lambda \abs{z_2}\\ 
\end{pmatrix}
\end{equation*}
Therefore $\lambda$ is an eigenvalue of both $X$ and $Y$, in contradiction to our assumption that it has simple index.
\end{proof}

\section{A closer look at the Perron-Frobenius theorem}

\begin{problem}{4.1}\label{problem4.1}
Prove that if $0 \neq x \in \mathcal{E}_A(\lambda_1)$ then $x_i = 0$ for no  $i \in [n]$
\end{problem}

\begin{proof}
Let $x \in \mathcal{E}_A(\lambda_1)$. Then $\lambda_1 = x^TAx$ by the Rayleigh principle \cite{rayleigh}. Set $y$ to be the vector 
\begin{equation*}
y_i = \abs{x_i} \text{ for every i } \in [n]
\end{equation*}
Notice that
\begin{equation*}
\abs{\abs{y}}^2 = \sum_i y_i^2 = \sum_i x_i^2 = \abs{\abs{x}}^2 
\end{equation*}

Then
\begin{align*}
\begin{split}
\lambda_1 &=  x^TAx \\
				 &= \sum_{ij} A_{ij}x_ix_j\\
				 &\leq \sum_{ij} A_{ij}\abs{x_i}\abs{x_j}\\
				 &= \sum_{ij} A_{ij}y_iy_j\\
				 &=  y^TAy \\
				 &\leq \lambda_1
\end{split}
\end{align*}
Recall that $\lambda_1 = \underset{\abs{\abs{x}} = 1}{\text{max }} x^TAx$, therefore if $x$ is in the eigenspace of $\lambda_1$, so is $y$ (as they both got same length, and  $y^TAy \geq x^TAx$).
It remains to show that $y_i = 0$ for $i \in [n]$. Suppose by contradiction that there is some $y_i = 0$ for $i \in [n]$,
and $y_j \neq 0$ for some $j \in [n]$, such $j$ must exists because $\abs{\abs{y}} = \abs{\abs{x}} \text{ and }x \neq 0$.
We saw in \cite{adjacency_matrix} that
\begin{equation*}
0 \neq y \in \mathcal{E}_A(\lambda_1) \text{ then } \lambda_1 y_u = \sum_{v\sim u} y_v
\end{equation*}
Let P be an $i-j$ path in G. because $y_i = 0$ and $y_j > 0$ there must be an edge in P such that $y_k = 0$ and $y_l > 0$
\begin{equation*}
0 = \lambda_1 y_k = \sum_{v\sim k} y_v \geq y_l > 0
\end{equation*}
\end{proof}

\begin{problem}{4.2}\label{problem4.2}
Use the assertion of Problem \ref{problem4.1} in order to prove that $\lambda_1$ has multiplicity $1$
Once we have that dim $\mathcal{E}_A(\lambda_1) = 1$ the following is trivial.
\end{problem}

\begin{proof}
By contradiction assume that $\lambda_1$ has multiplicity bigger than $1$,
let $x_1,x_2 \in \mathcal{E}_A(\lambda_1)$ be independent (otherwise they will represent the same vector), by \ref{problem4.1} all elements in $x_1,x_2 > 0$ and so we can define a new vector $x$ using $0 \neq c \in \N$ such that $0 \neq x = x_1 - c x_2$ and at least one of $x$ element is equal zero.
Recall that the matrix product is distributive \cite{matrix_multiplication}, now notice that 
\begin{equation*}
 Ax = A(x_1 - c x_2) = Ax_1 - cAx_2= \lambda_1 x_1 - c\lambda_1 x_2 = \lambda_1 (x_1 -cx_2) = \lambda_1 x
\end{equation*}
So $x \in  \mathcal{E}_A(\lambda_1)$ in contradiction to \ref{problem4.1} (no zero elements).
\end{proof}

\begin{problem}{4.3}\label{problem4.3}
Let $0 \neq x \in \mathcal{E}_A(\lambda_1)$. Prove that all entries of $x$ have the same sign.
\end{problem}

\begin{proof}
Let $x \in \mathcal{E}_A(\lambda_1)$. Set $y$ to be the vector 
\begin{equation*}
y_i = \abs{x_i} \text{ for every i } \in [n]
\end{equation*}
We saw in \ref{problem4.1} that  $y \in \mathcal{E}_A(\lambda_1)$ and that there is no zero elements in $x$, we saw also in \ref{problem4.2} that the eigenspace of $\lambda_1$ has multiplicity 1, therefore all sign must be equal. 
\end{proof}

\begin{problem}{4.4}\label{problem4.4}
Suppose that $x \in \mathcal{E}_A(\lambda)$ for some eigenvalue $\lambda$ of $A$ and that $x_i > 0$ for every $i \in [n]$.
Prove that $\lambda = \lambda_1$.
\end{problem}

\begin{proof}
Let $x_1 \in \mathcal{E}_A(\lambda_1)$ , from \ref{problem4.2} we know that the eigenspace of $\lambda_1$ has multiplicity 1  and from \cite{perron_frobenius} we know that the eigenspace of $\lambda_1$ contain a vector that all of his elements are bigger than zero.
now assume that $\lambda \neq \lambda_1$, from \cite{orthogonal_eigenvectors} we know that all of the eigenvectors are orthogonal and so $x^Tx_1 = 0$ therefore x must contain negative value.
\end{proof}

\begin{thebibliography}{9} 
\bibitem{konig_egervary}
Elad Aigner-Horev,
The K$\ddot{o}$nig-Egerv$\acute{a}$ry theorem.
\\\texttt{http://www.elad-horev.org/Expositions/Konig\_Egervary.pdf}

\bibitem{adjacency_matrix}
Elad Aigner-Horev,
The K$\ddot{o}$nig-Egerv$\acute{a}$ry theorem.
\\\texttt{http://www.elad-horev.org/Expositions/Adjacency\_Matrix.pdf}

\bibitem{determinant}
Determinant.
\\\texttt{https://en.wikipedia.org/wiki/Determinant}

\bibitem{connectivity_spec}
Elad Aigner-Horev,
Connectivity and spectrum.
\\\texttt{http://www.elad-horev.org/Teaching/GT/Assignments/connectivity\_spec.pdf}

\bibitem{basic_info_spec}
Elad Aigner-Horev,
Information from the spectrum of the adjacency matrix.
\\\texttt{http://www.elad-horev.org/Expositions/basic\_info\_spec.pdf}

\bibitem{connectivity}
Connectivity.
\\\texttt{https://en.wikipedia.org/wiki/Connectivity\_(graph\_theory)}

\bibitem{perron_frobenius}
Elad Aigner-Horev,
\\\texttt{http://www.elad-horev.org/Expositions/Perron\_Frobenius.pdf}

\bibitem{rayleigh}
Elad Aigner-Horev,
\\\texttt{http://www.elad-horev.org/Expositions/Rayleigh.pdf}

\bibitem{matrix_multiplication}
Matrix multiplication.
\\\texttt{https://en.wikipedia.org/wiki/Matrix\_multiplication}

\bibitem{orthogonal_eigenvectors}
Eigenvectors of real symmetric matrices are orthogonal.
\begin{scriptsize}
\\\texttt{https://math.stackexchange.com/questions/82467/eigenvectors-of-real-symmetric-matrices-are-orthogonal}
\end{scriptsize}
\end{thebibliography}

\end{document}